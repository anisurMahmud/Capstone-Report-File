\chapter{Testing and Result Analysis}
This chapter emphasizes the testing and result analysis procedure of our model. Testing and result analysis are vital component of the development and assessment process for any model. Testing is a systematic procedure of evaluating a model's performance based on various measures. Where the goal is to ensure that the model meets it's expected requirements. On the other hand the result analysis involves analysing the outcome of the testing process. This process interprets  model's performance and effectiveness in various metrics and calculations.

\section{Testing Procedure}
About four thousands of text data are used to test the performance of our built model. Where two thousands are used as validation data and another two thousands are used as test data. Here totally different text data are used for finding out the actual performance of our model.

\section{Model Evaluation}
Model evaluation is a process which help us to measure the performance of a model based on some metrics. This procedure in needed for a model to check whether the model meets the expected performance or not. This also helps to find out the weakness of a model. There are many types of metrics for evaluating a model. Such as Accuracy, Precision, Recall , F1 score etc. This section emphasizes some of the performance measure metrics which are used to evaluate the performance of our model.
\subsection{Confusion Matrix}
Most of the performance metrics are measured based on the confusion matrix. It's a two dimensional table which consists of actual value and predicted value. And four types of value are represented in this table based on these two types of value. Which are True Positive(TP), False Positive(FP), False Negative(FN) and True Negative(TN). True positive and true negative represents the term when actual value and predicted value are same. Where false positive and false negative represents the mismatch between actual value and predicted value.

\begin{table}[h!]
\centering
\begin{tabular}{|l|l|l|}
\hline
& Actual Positive (1) & Actual Negative (0)\\
\hline
Predicted Positive (1) & True Positive (TP) & False Positive (FP)  \\
\hline
Predicted Negative (0) & False negative(FN) & True Positive(TP) \\
\hline
\end{tabular}
\caption{Confusion Matrix}
\end{table}

\subsection{Accuracy}
Accuracy represents the model's correct predictions in a mathematical form. It defines how close the prediction to the actual value.
The formula for calculating accuracy is,
\begin{equation}
Accuracy = {\frac{TP+TN}{TP+TN+FP+FN}}
\end{equation}

\subsection{Precision}
Precision represents how close the measure value to each other. Here the value of true positives are divided by the total predicted positives,
\begin{equation}
Precision = {\frac{TP}{TP+FP}}
\end{equation}

\subsection{Recall}
Recall defines the ratio of all correctly predicted positive predictions. Here the value of true positive predictions are divided by all positive predictions.
\begin{equation}
Recall = {\frac{TP}{TP+FN}}
\end{equation}

\subsection{F1 Score}
F1 score represents the overall test accuracy of a model. It provides feedback based on precision and recall.
\begin{equation}
F1 Score = {\frac{2*(Precision*Recall)}{Precision+Recall}}
\end{equation}

\section{Results And Discussion}
We use a data-set for the implementation of our LSTM architecture is mainly text based contained total 20000 thousand plain anonymous messages. We investigated our architecture in two ways. First we ran the model without using Word Embedding, then we found improvements in training, testing and validation accuracy using GloVe Embedding. LSTM architecture gives 0.8245 as Testing Accuracy without using Embedding while using Embedding the architecture gives 0.9255. Also, we have greatly reduced training and validation losses by using GloVe. The proposed model is trained with a limit of 70 epochs.

\begin{table}[h!]
\centering
\begin{tabular}{|c|cc|cc|cc|}
\hline
\multirow{2}{*}{Model} & \multicolumn{2}{c|}{Training}         & \multicolumn{2}{c|}{Validation}       & \multicolumn{2}{c|}{Testing}          \\ \cline{2-7} 
                       & \multicolumn{1}{c|}{Accuracy} & Loss  & \multicolumn{1}{c|}{Accuracy} & Loss  & \multicolumn{1}{c|}{Accuracy} & Loss  \\ \hline
LSTM Without GloVe     & \multicolumn{1}{c|}{92.1\%}   & 1.9\% & \multicolumn{1}{c|}{83.2\%}   & 4.2\% & \multicolumn{1}{c|}{82.45\%}  & 4.5\% \\ \hline
LSTM With GloVe        & \multicolumn{1}{c|}{97.8\%}   & 0.5\% & \multicolumn{1}{c|}{93.1\%}   & 1.7\% & \multicolumn{1}{c|}{92.55\%}  & 1.8\% \\ \hline
\end{tabular}
\caption{Model Accuracy Comparison}
\end{table}

Table 5.2. The table shows the model accuracy comparison while using and not using GloVe with the model. So, to get better results using Word Embedding with LSTM found more effective.

We have worked with 6 emotions in our model. Precision, recall, F1-scores for different emotions are shown in Table 5.3.


\pagebreak

\begin{table}[h!]
\centering
\begin{tabular}{|c|c|c|c|}
\hline
Emotion       & Precision & Recall & F1-Score \\ \hline
Sadness       & 0.92      & 0.94   & 0.93     \\ \hline
Anger         & 0.88      & 0.90   & 0.89     \\ \hline
Love          & 0.92      & 0.96   & 0.94     \\ \hline
Surprise      & 0.91      & 0.77   & 0.84     \\ \hline
Fear          & 0.97      & 0.96   & 0.96     \\ \hline
Joy           & 0.79      & 0.73   & 0.76     \\ \hline
Accuracy      & -         & -      & 0.93     \\ \hline
Marco Avg.    & 0.90      & 0.88   & 0.89     \\ \hline
Weighted Avg. & 0.93      & 0.93   & 0.92     \\ \hline
\end{tabular}
\caption{The results obtained for Six Emotions}
\end{table}

Table 5.3. The Table shows among the 6 emotions,  the F1-Score of 'Fear' is the highest, the scores of other emotions are also close. F1-score's accuracy 0.93. F1-score's macro avg. and weighted avg. 0.89 and 0.92.